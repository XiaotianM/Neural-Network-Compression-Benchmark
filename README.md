# Neural Network Compression Benchmark


## Small and Efficient Networks：
- SqueezeNet
- MobileNet 
- ShuffleNet
- IGC V2
- MobileNet V2
- SqueezeNeXt
- IGC V3
 - [Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution](https://arxiv.org/abs/1904.05049),2019，颜水成组， 一种结合高低频的可替代传统卷积的OctaveConv


## Network pruning

- [Data-driven sparse structure selection for deep neural networks, 图森, ECCV2018](https://arxiv.org/abs/1707.01213)

- [DAC: Data-free Automatic Acceleration of Convolutional Networks, Xin Li, et.al, 2018
](https://arxiv.org/abs/1812.08374?context=cs)
该文章提出无需重新训练，对预训练完成的模型进行离线转换成Depthwise Separable Convolution

-[MorphNet: Fast & Simple Resource-Constrained Learning of Deep Network Structure](https://arxiv.org/abs/1711.06798), CVPR2018, Google发布的对网络自动裁剪模型。 [code](https://arxiv.org/abs/1711.06798)

## Network quantization

## Knowledge Transfer:
- [Distilling the Knowledge in a Neural Network, Hinton et al, NIPS 2014](https://arxiv.org/abs/1503.02531)

- [FitNets- Hints for Thin Deep Nets, ICLR 2015](https://arxiv.org/abs/1412.6550)

- [Like What You Like: Knowledge Distill via Neuron Selectivity Transfer, Zehao Huang et.al, 2017](https://arxiv.org/abs/1707.01219)
图森TuSimple的工作

